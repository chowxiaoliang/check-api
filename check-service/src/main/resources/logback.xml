<?xml version="1.0" encoding="UTF8"?>

<configuration>

	<substitutionProperty name="max.size" value="100MB" />
	<contextName>checkapi</contextName>
	<jmxConfigurator />

	<appender name="stdout" class="ch.qos.logback.core.ConsoleAppender">
		<layout class="ch.qos.logback.classic.PatternLayout">
			<pattern>[%date{yyyy-MM-dd HH:mm:ss.SSS}][%level][%thread][%logger{80}|%method|%line]%msg%n</pattern>
		</layout>
	</appender>

	<appender name="file.info" class="ch.qos.logback.core.rolling.RollingFileAppender"><!-- All Log Info -->
		<File>logs/info.log</File>
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<FileNamePattern>logs/bak_info_%d{yyyy-MM-dd}.%i.log.zip
			</FileNamePattern>
			<timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
				<maxFileSize>100MB</maxFileSize>
			</timeBasedFileNamingAndTriggeringPolicy>
			<MaxHistory>10</MaxHistory>
		</rollingPolicy>
		<encoder>
			<immediateFlush>false</immediateFlush>
			<pattern>[%date{yyyy-MM-dd HH:mm:ss.SSS}][%level][%thread][%logger{16}]%msg%n</pattern>
		</encoder>
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>INFO</level>
		</filter>
	</appender>

	<appender name="file.error" class="ch.qos.logback.core.rolling.RollingFileAppender">
		<File>logs/error.log</File>
		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<FileNamePattern>logs/bak_error_%d{yyyy-MM-dd}.%i.log.zip
			</FileNamePattern>
			<timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
				<maxFileSize>100MB</maxFileSize>
			</timeBasedFileNamingAndTriggeringPolicy>
			<MaxHistory>30</MaxHistory>
		</rollingPolicy>
		<encoder>
			<immediateFlush>true</immediateFlush>
			<pattern>[%date{yyyy-MM-dd HH:mm:ss.SSS}][%level][%thread][%logger|%method|%line]%msg%n</pattern>
		</encoder>
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>WARN</level>
		</filter>
	</appender>

	<!-- kafaka appender -->
	<appender name="kafka.async" class="com.bqs.ares.logback.kafka.LogbackKafkaAppender">
		<encoder class="com.bqs.ares.logback.kafka.KafkaEncoder">
			<!-- Kafaka服务器的brokers,使用","分割 -->
			<brokerList>${p.metadata.broker.list}</brokerList>
			<!-- 当前消息的topic，建议每个 -->
			<topic>app_log_topic</topic>
			<!-- 每条日志为100字节。设置为100W时，约等于100M内存 -->
			<bufferSize>2000000</bufferSize>
			<!-- 日志来源系统 -->
			<srcSys>checkapi</srcSys>
			<!-- 日志来源模块 -->
			<srcModule>checkapi</srcModule>
		</encoder>
		<filter class="ch.qos.logback.classic.filter.ThresholdFilter">
			<level>INFO</level>
		</filter>
	</appender>

	<!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 -->
	<!-- 更改默认的队列的深度,该值会影响性能.默认值为256 -->
	<!-- 添加附加的appender,最多只能添加一个 -->
	<appender name="file.info.async" class="ch.qos.logback.classic.AsyncAppender">
		<discardingThreshold>0</discardingThreshold>
		<queueSize>512</queueSize>
		<includeCallerData>true</includeCallerData>
		<appender-ref ref="file.info" />
	</appender>
	<appender name="file.error.async" class="ch.qos.logback.classic.AsyncAppender">
		<discardingThreshold>0</discardingThreshold>
		<queueSize>512</queueSize>
		<includeCallerData>true</includeCallerData>
		<appender-ref ref="file.error" />
	</appender>


	<root level="INFO">
		<appender-ref ref="stdout" /> 
		<appender-ref ref="file.info.async" />
		<appender-ref ref="file.error.async" />
	</root>

</configuration>

